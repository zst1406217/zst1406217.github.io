<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shaoting Zhu</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/shaotingzhu.jpg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shaoting Zhu (朱少廷)
                </p>
                <p>Hey, I'm Shaoting, a first-year PhD student at <a href="https://iiis.tsinghua.edu.cn/en/"><b>Institute for Interdisciplinary Information Sciences (IIIS)</b></a>, <b>Tsinghua University</b>, majoring in <b>Computer Science</b>. I am very fortunate to be advised by <a href="https://hangzhaomit.github.io/">Prof. Hang Zhao</a> in <a href="http://group.iiis.tsinghua.edu.cn/~marslab/">MARS Lab</a>. Before that, I received my bachelor's degree from Zhejiang University. I was advised by <a href="https://scholar.google.com/citations?user=qYcgBbEAAAAJ&hl=en">Prof. Yong Liu</a> in <a href="https://april.zju.edu.cn/">APRIL Lab</a> from <a href="http://cse.zju.edu.cn/">CSE</a>.
                </p>
                <p>
                  I like to make handicrafts and listen to music in my spare time. Also, I love doing outdoor activities, especially traveling and road trip.
                </p>
                <p style="text-align:center">
                  <a href="mailto:zhust24@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=BE7NiFcAAAAJ&hl=zh-CN&oi=ao">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/ShaotingZ38103">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/zst1406217">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/shaotingzhu.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/shaotingzhu.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Recent News</h2>
              <div style="background-color: #E7F4FF; padding: 8px; border-radius: 4px; margin-top:18px">
                <ul style="margin: 0; padding: 0 0 0 20px;">
                  <li>
                    Two papers <a href="https://moe-loco.github.io/">MoELoco</a> and <a href="https://roboengine.github.io/">RoboEngine</a> accepted by IROS 2025!
                  </li>
                  <li>
                    One paper <a href="https://vr-robo.github.io/">VR-Robo</a> accepted by RA-L 2025!
                  </li>
                  <li>
                    Two papers <a href="https://saro-vlm.github.io/">SARO</a> and <a href="https://robust-robot-walker.github.io/">RRW</a> accepted by ICRA 2025!
                  </li>
                  <li>
                    I'm currently pursuing my PhD at Tsinghua University! (since Fall 2024)
                  </li>
                </ul>
              </div>
            </td>
          </tr>
        </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:-25px;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interest includes <b>Robotics</b>, <b>Computer Vision</b> and <b>Artificial Intelligence</b>. (*indicates equal contribution)
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr>
              <td style="padding:16px;width:40%;vertical-align:middle">
                <div id="vrrobo_container" style="width:100%; aspect-ratio: 16 / 9; overflow:hidden; position:relative;">
                  <video id="vrrobo_video" muted autoplay loop playsinline
                        style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover;">
                    <source src="images/vr-robo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://vr-robo.github.io/">
                  <span class="papertitle">VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion</span>
                </a>
                <br>
                <strong>Shaoting Zhu*</strong>,
                <a href="https://linzhanm.github.io/">Linzhan Mou*</a>,
                <a href="https://scholar.google.com/citations?user=oSnm3PkAAAAJ&hl=en">Derun Li</a>,
                <a href="https://scholar.google.cz/citations?user=u-ELP2oAAAAJ&hl=zh-CN">Baijun Ye</a>,
                <a href="https://hrh6666.github.io/">Runhan Huang</a>,
                <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                <br>
                <em>RA-L</em>, 2025
                <br>
                <a href="https://vr-robo.github.io/">project page</a> /
                <a href="https://www.youtube.com/watch?v=CcTCECSCwz4&feature=youtu.be">video</a> /
                <a href="https://arxiv.org/abs/2502.01536">arXiv</a> /
                <a href="https://github.com/zst1406217/VR-Robo">code</a>
                <p></p>
                <p>
                  VR-Robo introduces a digital twin framework using 3D Gaussian Splatting for photorealistic simulation, enabling RGB-based sim-to-real transfer for robot navigation and locomotion.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:40%;vertical-align:middle">
                <div id="moeloco_container" style="width:100%; aspect-ratio: 16 / 9; overflow:hidden; position:relative;">
                  <video id="moeloco_video" muted autoplay loop playsinline
                         style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover;">
                    <source src="images/moeloco.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://moe-loco.github.io/">
                  <span class="papertitle">MoELoco: Mixture of Experts for Multitask Locomotion</span>
                </a>
                <br>
                <a href="https://hrh6666.github.io/">Runhan Huang*</a>,
                <strong>Shaoting Zhu*</strong>,
                <a href="https://yilundu.github.io/">Yilun Du</a>,
                <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                <br>
                <em>IROS, 2025</em> 
                <br>
                <a href="https://moe-loco.github.io/">project page</a> /
                <a href="https://www.youtube.com/watch?v=1zXk7IeruVM">video</a> /
                <a href="https://arxiv.org/abs/2503.08564">arXiv</a>
                <p></p>
                <p>
                  MoELoco introduces a multitask locomotion framework that employs a mixture-of-experts strategy to enhance reinforcement learning across diverse tasks while leveraging compositionality to generate new skills.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:40%;vertical-align:middle">
                <div id="roboengine_container" style="width:100%; aspect-ratio: 16 / 9; overflow:hidden; position:relative;">
                  <video id="roboengine_video" muted autoplay loop playsinline
                         style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover;">
                    <source src="images/roboengine.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://roboengine.github.io/">
                  <span class="papertitle">RoboEngine: Plug-and-Play Robot Data Augmentation with Semantic Robot Segmentation and Background Generation</span>
                </a>
                <br>
                <a href="https://michaelyuancb.github.io/">Chengbo Yuan*</a>,
                <a href="/#">Suraj Joshi*</a>,
                <strong>Shaoting Zhu*</strong>,
                <a href="#">Hang Su</a>,
                <a href="https://hangzhaomit.github.io/">Hang Zhao</a>,
                <a href="http://people.iiis.tsinghua.edu.cn/~gaoyang/">Yang Gao</a>
                <br>
                <em>IROS</em>, 2025
                <br>
                <a href="https://roboengine.github.io/">project page</a> /
                <a href="https://youtu.be/6j4z6i0SVu4">video</a> /
                <a href="https://arxiv.org/abs/2503.18738">arXiv</a> /
                <a href="https://github.com/michaelyuancb/roboengine">code</a>
                <p></p>
                <p>
                  RoboEngine is the first plug-and-play visual robot data augmentation toolkit. Users can effortlessly generate physics-aware robot scenes with few lines code. This enable training only in one scenes and visual generalizing to almost arbitrary scenes.                </p>
              </td>
            </tr>


            <tr>
              <td style="padding:16px;width:40%;vertical-align:middle">
                <div id="rrw_container" style="width:100%; aspect-ratio: 16 / 9; overflow:hidden; position:relative;">
                  <video id="rrw_video" muted autoplay loop playsinline
                         style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover;">
                    <source src="images/rrw.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://robust-robot-walker.github.io/">
                  <span class="papertitle">Robust Robot Walker: Learning Agile Locomotion over Tiny Traps</span>
                </a>
                <br>
                <strong>Shaoting Zhu</strong>,
                <a href="https://hrh6666.github.io/">Runhan Huang</a>,
                <a href="https://linzhanm.github.io/">Linzhan Mou</a>,
                <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                <br>
                <em>ICRA</em>, 2025
                <br>
                <a href="https://robust-robot-walker.github.io/">project page</a> /
                <a href="https://www.youtube.com/watch?v=zoYPVh1ejMU">video</a> /
                <a href="https://arxiv.org/abs/2409.07409">arXiv</a> /
                <a href="https://github.com/zst1406217/robust_robot_walker">code</a>
                <p></p>
                <p>
                  We propose a proprioception-only, two-stage training framework with goal command and a dedicated tiny trap benchmark, enabling quadruped robots to robustly traverse small obstacles.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:40%;vertical-align:middle">
                <div id="saro_container" style="width:100%; aspect-ratio: 16 / 9; overflow:hidden; position:relative;">
                  <video id="saro_video" muted autoplay loop playsinline
                         style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover;">
                    <source src="images/saro.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://saro-vlm.github.io/">
                  <span class="papertitle">SARO: Space-Aware Robot System for Terrain Crossing via Vision-Language Model</span>
                </a>
                <br>
                <strong>Shaoting Zhu*</strong>,
                <a href="https://scholar.google.com/citations?user=oSnm3PkAAAAJ&hl=en">Derun Li*</a>,
                <a href="https://linzhanm.github.io/">Linzhan Mou</a>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=qYcgBbEAAAAJ">Yong Liu</a>,
                <a href="#">Ningyi Xu</a>,
                <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                <br>
                <em>ICRA</em>, 2025
                <br>
                <a href="https://saro-vlm.github.io/">project page</a> /
                <a href="https://youtu.be/vfsAgrhWkp0">video</a> /
                <a href="https://arxiv.org/abs/2407.16412">arXiv</a>
                <p></p>
                <p>
                  SARO is an innovative system composed of a high-level reasoning module, a closed-loop sub-task execution module, and a low-level control policy. It enables the robot to navigate across 3D terrains and reach the goal position.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:40%;vertical-align:middle">
                <div id="T5_container" style="width:100%; aspect-ratio: 16 / 9; overflow:hidden; position:relative;">
                  <video id="T5_video" muted autoplay loop playsinline
                        poster="images/T5.jpg"
                         style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover;">
                    <source src="images/T5.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://openreview.net/pdf?id=TtGONY7UKy">
                  <span class="papertitle">T5-ARC: Test-Time Training for Transductive Transformer Models in ARC-AGI Challenge</span>
                </a>
                <br>
                <strong>Shaoting Zhu*</strong>,
                <a href="#">Shuangyue Geng*</a>,
                <a href="#">Un Lok Chen*</a>
                <br>
                <em>Course Project</em>, Advanced Machine Learning by <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a>
                <br>
                <a href="https://openreview.net/pdf?id=TtGONY7UKy">paper</a>
                <p></p>
                <p>
                  We focus on Test-Time Training (TTT) for transductive models and develop our pipeline following the SOTA methods, which consists of three steps: Base Model Training, TTT and Active Inference.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:40%;vertical-align:middle">
                <div id="Uni++_container" style="width:100%; aspect-ratio: 16 / 9; overflow:hidden; position:relative;">
                  <video id="Uni++_video" muted autoplay loop playsinline
                        poster="images/Uni++.png"
                         style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover;">
                    <source src="images/Uni++.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://link.springer.com/article/10.1007/s11263-025-02395-6">
                  <span class="papertitle">UniFace++: Revisiting a Unified Framework for Face Reenactment and Swapping via 3D Priors</span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=zlq2S_0AAAAJ&hl=zh-CN&oi=sra">Chao Xu</a>,
                <a href="https://scholar.google.com/citations?user=eKzoLC8AAAAJ&hl=zh-CN&oi=sra">Yijie Qian</a>,
                <strong>Shaoting Zhu</strong>,
                <a href="https://scholar.google.com/citations?user=ZNhTHywAAAAJ&hl=zh-CN&oi=sra">Baigui Sun</a>,
                <a href="https://scholar.google.com/citations?user=yYpQrj0AAAAJ&hl=zh-CN&oi=sra">Jian Zhao</a>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=qYcgBbEAAAAJ">Yong Liu</a>,
                <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN&oi=ao">Xuelong Li</a>
                <br>
                <em>IJCV</em>, 2025
                <br>
                <a href="https://link.springer.com/article/10.1007/s11263-025-02395-6">paper</a>
                <p></p>
                <p>
                  UniFace++ combines the advantages of each, ie, stability of reconstruction training from reenactment, simplicity and effectiveness of the target-oriented processing from swapping, and redefining both as target-oriented reconstruction tasks.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:40%;vertical-align:middle">
                <div id="Uni++_container" style="width:100%; aspect-ratio: 16 / 9; overflow:hidden; position:relative;">
                  <video id="Uni++_video" muted autoplay loop playsinline
                        poster="images/Multi.png"
                         style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover;">
                    <source src="images/Multi.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2305.02594">
                  <span class="papertitle">Multimodal-driven talking face generation via a unified diffusion-based generator</span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=zlq2S_0AAAAJ&hl=zh-CN&oi=sra">Chao Xu</a>,
                <strong>Shaoting Zhu</strong>,
                <a href="https://scholar.google.com/citations?user=-OxQlHsAAAAJ&hl=zh-CN&oi=sra">Junwei Zhu</a>,
                <a href="https://scholar.google.com/citations?user=Fg7WYfcAAAAJ&hl=zh-CN&oi=sra">Tianxin Huang</a>,
                <a href="https://scholar.google.com/citations?user=2hA4X9wAAAAJ&hl=zh-CN&oi=sra">Jiangning Zhang</a>,
                <a href="https://scholar.google.com/citations?user=NKaiUasAAAAJ&hl=zh-CN&oi=sra">Ying Tai</a>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=qYcgBbEAAAAJ">Yong Liu</a>
                <br>
                <em>arXiv</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2305.02594">paper</a>
                <p></p>
                <p>
                  Given a textured face as the source and the rendered face projected from the desired 3DMM coefficients as the target, our proposed Texture-Geometry-aware Diffusion Model decomposes the complex transfer problem into multi-conditional denoising process.</p>
              </td>
            </tr>

          </tbody></table>

					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <!-- <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr> -->


            <!-- <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr> -->

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://iros25.org/">Reviewer, IROS 2025</a>
                <br>
                <a href="https://www.ieee-ras.org/publications/ram">Reviewer, RA-M</a>
                <br>
                <a href="https://www.ieee-ras.org/publications/ra-l">Reviewer, RA-L</a>
              </td>
            </tr>
					
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                Teaching Assistant, Advances in Autonomous Driving and Intelligent Vehicles, Fall 2024
              </td>
            </tr>
                        <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Education</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                2020-2024: B.S. in <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>, Hangzhou, China. 
                <br>
                Honored with Chu Kochen Scholarship in 2023.
                <br>
                2024-Now: Ph.D. in <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, Beijing, China.
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Template borrowed from <a href="https://github.com/jonbarron/jonbarron_website">jonbarron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
